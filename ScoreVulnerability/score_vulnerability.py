
import os
import glob
import random
import numpy as np
from tqdm import tqdm

import torch

from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
)

SEED=42

random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)
torch.cuda.manual_seed(SEED)
torch.cuda.manual_seed_all(SEED)

model_id = "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B"

model = AutoModelForCausalLM.from_pretrained(
    model_id,
    torch_dtype="auto",
    device_map="auto",
)
tokenizer = AutoTokenizer.from_pretrained(model_id)

system_prompt = open("./prompt.txt", 'r').read()
user_prompt = f"```c\n{open('./in/png.c.vul.clean', 'r').read()}\n```"

messages = [
    {"role": "system", "content": system_prompt},
    {"role": "user", "content": user_prompt}
]

inputs_text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True,
    enable_thinking=True,
)

model_inputs = tokenizer(
    inputs_text,
    return_tensors="pt",
).to(model.device)
    
generated_ids = model.generate(
    **model_inputs,
    max_new_tokens=10000,
    do_sample=False
)

generated_ids = [
    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
]

response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]

os.makedirs("./out", exist_ok=True)
with open("./out/png.c.vul.clean", 'w') as f:
    f.write(response)

print("process succeed!")

