
import os
import sys
from openai import OpenAI

# vLLM 서버 설정 (Session 1에서 실행한 주소)
client = OpenAI(
    base_url="http://localhost:8000/v1",
    api_key="EMPTY"  # vLLM은 로컬 실행 시 API 키가 필요 없습니다.
)

# 모델 ID (서버 실행 시 사용한 이름과 동일해야 함)
model_id = "Qwen/Qwen3-Coder-30B-A3B-Instruct"

def main():
    print("[*] Reading inputs...")
    try:
        system_prompt = open("./prompt.txt", 'r', encoding='utf-8').read()
        code_content = open('./in/png.c.vul', 'r', encoding='utf-8').read()
        user_prompt = f"```c\n{code_content}\n```"
    except FileNotFoundError as e:
        print(f"[!] Error reading input files: {e}")
        return

    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_prompt}
    ]

    print("[*] Sending request to vLLM server...")
    
    try:
        response = client.chat.completions.create(
            model=model_id,
            messages=messages,
            max_tokens=10000,     # 출력 길이 제한
            temperature=0.0,      # do_sample=False와 동일 (Greedy Decoding)
            # extra_body={"stop_token_ids": [tokenizer.eos_token_id]} # 필요 시 추가
        )

        output_text = response.choices[0].message.content
        
        # vLLM은 chat template을 자동으로 처리하므로 <think> 태그 등은 
        # 모델 학습 방식에 따라 출력에 포함될 수 있습니다.

    except Exception as e:
        print(f"[!] API Request failed: {e}")
        print("    -> vLLM 서버가 켜져 있는지(Session 1) 확인하세요.")
        return

    print("[*] Saving output...")
    os.makedirs("./out", exist_ok=True)
    with open("./out/png.c.vul.response", 'w', encoding='utf-8') as f:
        f.write(output_text)

    print("process succeed!")

if __name__ == "__main__":
    main()

